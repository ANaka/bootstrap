{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandernaka/Library/Caches/pypoetry/virtualenvs/bootstrap-TdbWQQRJ-py3.10/lib/python3.10/site-packages/langchain/llms/openai.py:169: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/alexandernaka/Library/Caches/pypoetry/virtualenvs/bootstrap-TdbWQQRJ-py3.10/lib/python3.10/site-packages/langchain/llms/openai.py:608: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.agents import initialize_agent, Tool, tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from bootstrap.toolchain import get_tools\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Construct the agent. We will use the default agent type here.\n",
    "# See documentation for a full list of options.\n",
    "agent = initialize_agent(\n",
    "    tools=get_tools(),\n",
    "    llm=ChatOpenAI(temperature=0.7, model_name='gpt-4', ), \n",
    "    agent=\"conversational-react-description\", \n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Langchain repo query\n",
      "Action Input: What are the valid arguments for the `agent` parameter of the `initialize_agent` function in the langchain.agents module?\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 9756 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 28 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m\n",
      "\n",
      "The valid arguments for the `agent` parameter of the `initialize_agent` function in the langchain.agents module are the names of the agent classes defined in the module, such as `RandomAgent`, `GreedyAgent`, `QAgent`, `zero-shot-react-description`, `react-docstore`, `self-ask-with-search`, and `conversational-react-description`, as well as any other custom agents that have been defined in the `tests/unit_tests/agents/__init__.py` file. These classes are described in the `Reference <../reference/modules/agents.rst>`_ section of the documentation. Additionally, agents can be constructed using toolkits, which are groups of tools that are best used together. Toolkits allow you to logically group and initialize a set of tools that share a particular resource (such as a database connection or json object). Toolkits are defined in the `__all__` list of the `langchain.agents.__init__.py` file, and include functions such as `create_sql_agent`, `create_json_agent`, `create_openapi_agent`, `create_vectorstore\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "AI: The valid arguments for the `agent` parameter of the `initialize_agent` function in the langchain.agents module are the names of the agent classes defined in the module, such as `RandomAgent`, `GreedyAgent`, `QAgent`, `zero-shot-react-description`, `react-docstore`, `self-ask-with-search`, and `conversational-react-description`. Additionally, agents can be constructed using toolkits, which are groups of tools that are best used together. Toolkits allow you to logically group and initialize a set of tools that share a particular resource (such as a database connection or JSON object). Toolkits are defined in the `__all__` list of the `langchain.agents.__init__.py` file and include functions such as `create_sql_agent`, `create_json_agent`, `create_openapi_agent`, and `create_vectorstore`.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The valid arguments for the `agent` parameter of the `initialize_agent` function in the langchain.agents module are the names of the agent classes defined in the module, such as `RandomAgent`, `GreedyAgent`, `QAgent`, `zero-shot-react-description`, `react-docstore`, `self-ask-with-search`, and `conversational-react-description`. Additionally, agents can be constructed using toolkits, which are groups of tools that are best used together. Toolkits allow you to logically group and initialize a set of tools that share a particular resource (such as a database connection or JSON object). Toolkits are defined in the `__all__` list of the `langchain.agents.__init__.py` file and include functions such as `create_sql_agent`, `create_json_agent`, `create_openapi_agent`, and `create_vectorstore`.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run('what are valid arguments to the `agent` parameter of `langchain.agents.initialize_agent`?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: llama_index repo query\n",
      "Action Input: how to do a query using llama_index\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 8676 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 10 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m\n",
      "\n",
      "To query a Knowledge Graph Index using llama_index, you can use the query() method from the gpt_index.indices.query.knowledge_graph module. This module provides additional methods such as index_struct, set_llm_predictor, set_prompt_helper to help with the query. Additionally, you can use the query() method to ask specific questions about the risk factors described in the Uber 10-K for a particular fiscal year, such as the 2019 or 2020 fiscal year. You can also view info and/or debugging logging using the logging module, and save and load the index from disk. For more information on LlamaIndex features, please check out the numerous \"How-To Guides\" and Example Notebooks. Additionally, LlamaIndex offers different methods of synthesizing a response, such as Create and Refine and Tree Summarize. You can find more information on how to specify different response modes in the Usage Pattern Guide.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "AI: To query a Knowledge Graph Index using llama_index, you can use the `query()` method from the `gpt_index.indices.query.knowledge_graph` module. This module provides additional methods such as `index_struct`, `set_llm_predictor`, `set_prompt_helper` to help with the query. You can use the `query()` method to ask specific questions about the risk factors described in the Uber 10-K for a particular fiscal year, such as the 2019 or 2020 fiscal year.\n",
      "\n",
      "LlamaIndex also offers different methods of synthesizing a response, such as Create and Refine and Tree Summarize. You can find more information on how to specify different response modes in the Usage Pattern Guide.\n",
      "\n",
      "For more information on LlamaIndex features, please check out the numerous \"How-To Guides\" and Example Notebooks. Additionally, you can view info and/or debugging logging using the logging module, and save and load the index from disk.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To query a Knowledge Graph Index using llama_index, you can use the `query()` method from the `gpt_index.indices.query.knowledge_graph` module. This module provides additional methods such as `index_struct`, `set_llm_predictor`, `set_prompt_helper` to help with the query. You can use the `query()` method to ask specific questions about the risk factors described in the Uber 10-K for a particular fiscal year, such as the 2019 or 2020 fiscal year.\\n\\nLlamaIndex also offers different methods of synthesizing a response, such as Create and Refine and Tree Summarize. You can find more information on how to specify different response modes in the Usage Pattern Guide.\\n\\nFor more information on LlamaIndex features, please check out the numerous \"How-To Guides\" and Example Notebooks. Additionally, you can view info and/or debugging logging using the logging module, and save and load the index from disk.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run('how do I do a query using llama index?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootstrap-TdbWQQRJ-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
